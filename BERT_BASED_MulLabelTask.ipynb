{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wen2Tee5\\Desktop\\BERT\\bertEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_BASE_UNCASED = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 部分\n",
    "\n",
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self, TYPE = BERT_BASE_UNCASED) -> None:\n",
    "        super(BERT, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(BERT_BASE_UNCASED)\n",
    "\n",
    "    def forward(self, dict):\n",
    "        out = self.model(**dict)\n",
    "        cls_output = out.pooler_output\n",
    "        seq_output = out.last_hidden_state\n",
    "        return cls_output, seq_output\n",
    "\n",
    "class WordSelector(torch.nn.Module):\n",
    "    # 字詞選擇器\n",
    "    def __init__(self, d_model = 768) -> None:\n",
    "        super(WordSelector, self).__init__()\n",
    "        self.linear = torch.nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, last_state):\n",
    "        out = self.linear(last_state)\n",
    "        return out\n",
    "\n",
    "class SentPoistionTeller(torch.nn.Module):\n",
    "    # 立場辨識模組\n",
    "    def __init__(self, dim_q = 768, dim_r = 768) -> None:\n",
    "        super(SentPoistionTeller, self).__init__()\n",
    "        self.linear = torch.nn.Linear(dim_q + dim_r, 2)\n",
    "\n",
    "    def forward(self, q_cls, r_cls):\n",
    "        h = torch.concat([q_cls, r_cls], dim = 1)\n",
    "        out = self.linear(h)\n",
    "        return out\n",
    "\n",
    "class BertExtModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(BertExtModel, self).__init__()\n",
    "        self.r_bert = BERT(TYPE = BERT_BASE_UNCASED)\n",
    "        self.sent_position_teller = SentPoistionTeller()\n",
    "        self.word_selector = WordSelector()\n",
    "\n",
    "    def forward(self, q_dict, r_dict):\n",
    "        q_cls, q_last_seq = self.r_bert(q_dict)\n",
    "        r_cls, r_last_seq = self.r_bert(r_dict)\n",
    "        s = self.sent_position_teller(q_cls, r_cls)\n",
    "        q_out_seq = self.word_selector(q_last_seq)\n",
    "        r_out_seq = self.word_selector(r_last_seq)\n",
    "        return q_out_seq, r_out_seq, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('./PreprocessFullData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>com_q</th>\n",
       "      <th>com_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>1132</td>\n",
       "      <td>My argument that a `` human individual 's life...</td>\n",
       "      <td>Apologies that was n't my intention My meaning...</td>\n",
       "      <td>DISAGREE</td>\n",
       "      <td>human individual 's life biologically begins a...</td>\n",
       "      <td>your use of it as an unsupported 'fact forced ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>172</td>\n",
       "      <td>Give Ryuuichi a break will you She is a 17 yea...</td>\n",
       "      <td>Yes I was 17 years old before but if I asked a...</td>\n",
       "      <td>DISAGREE</td>\n",
       "      <td>She is a 17 year old girl that is contemplating</td>\n",
       "      <td>Now that I 'm 26 I can take people</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30701</th>\n",
       "      <td>8045</td>\n",
       "      <td>I would love to see more discussion of views t...</td>\n",
       "      <td>Yes it is interesting how we draw an arbitary ...</td>\n",
       "      <td>AGREE</td>\n",
       "      <td>I would love to see more discussion of views t...</td>\n",
       "      <td>it is interesting how we draw an arbitary `` l...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12425</th>\n",
       "      <td>3269</td>\n",
       "      <td>OK try the US Constitution We are a representa...</td>\n",
       "      <td>which is pretty much the same thing The majori...</td>\n",
       "      <td>DISAGREE</td>\n",
       "      <td>We are a representative democratic republic no...</td>\n",
       "      <td>The representatives are elected by a majority ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16715</th>\n",
       "      <td>4431</td>\n",
       "      <td>Pro-lifers are like slave traders What a pro-l...</td>\n",
       "      <td>Thats not the point The Pro-lifers advocate fo...</td>\n",
       "      <td>DISAGREE</td>\n",
       "      <td>Pro-lifers are like slave traders What a pro-l...</td>\n",
       "      <td>Thats not the point The Pro-lifers advocate fo...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                  q  \\\n",
       "4317   1132  My argument that a `` human individual 's life...   \n",
       "611     172  Give Ryuuichi a break will you She is a 17 yea...   \n",
       "30701  8045  I would love to see more discussion of views t...   \n",
       "12425  3269  OK try the US Constitution We are a representa...   \n",
       "16715  4431  Pro-lifers are like slave traders What a pro-l...   \n",
       "\n",
       "                                                       r         s  \\\n",
       "4317   Apologies that was n't my intention My meaning...  DISAGREE   \n",
       "611    Yes I was 17 years old before but if I asked a...  DISAGREE   \n",
       "30701  Yes it is interesting how we draw an arbitary ...     AGREE   \n",
       "12425  which is pretty much the same thing The majori...  DISAGREE   \n",
       "16715  Thats not the point The Pro-lifers advocate fo...  DISAGREE   \n",
       "\n",
       "                                                       Q  \\\n",
       "4317   human individual 's life biologically begins a...   \n",
       "611      She is a 17 year old girl that is contemplating   \n",
       "30701  I would love to see more discussion of views t...   \n",
       "12425  We are a representative democratic republic no...   \n",
       "16715  Pro-lifers are like slave traders What a pro-l...   \n",
       "\n",
       "                                                       R  \\\n",
       "4317   your use of it as an unsupported 'fact forced ...   \n",
       "611                   Now that I 'm 26 I can take people   \n",
       "30701  it is interesting how we draw an arbitary `` l...   \n",
       "12425  The representatives are elected by a majority ...   \n",
       "16715  Thats not the point The Pro-lifers advocate fo...   \n",
       "\n",
       "                                                   com_q  \\\n",
       "4317   [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "611    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "30701  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "12425   [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "16715                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                                   com_r  \n",
       "4317   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "611    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "30701  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "12425  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "16715  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.sample(5) # 前處理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_BASE_UNCASED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataFrame, tokenizer, padding = 'max_length', max_length = 512) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataframe = dataFrame\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # q_token = tokenizer.encode_plus(self.dataframe['q'].iloc[index].split(), padding = self.padding, max_length = self.max_length)\n",
    "        # r_token = tokenizer.encode_plus(self.dataframe['r'].iloc[index].split(), padding = self.padding, max_length = self.max_length)\n",
    "        q_token = tokenizer.encode_plus(self.dataframe['q'].iloc[index].split(), padding = self.padding, max_length = self.max_length, truncation=True)\n",
    "        r_token = tokenizer.encode_plus(self.dataframe['r'].iloc[index].split(), padding = self.padding, max_length = self.max_length, truncation=True)\n",
    "        s = 1 if self.dataframe['s'].iloc[index] == \"AGREE\" else 0\n",
    "        com_q = torch.tensor([1] + eval(self.dataframe['com_q'].iloc[index])[:self.max_length - 2] + [1])\n",
    "        com_r = torch.tensor([1] + eval(self.dataframe['com_r'].iloc[index])[:self.max_length - 2] + [1])\n",
    "        com_q = torch.nn.functional.pad(com_q, pad=(0, self.max_length - com_q.shape[0]))\n",
    "        com_r = torch.nn.functional.pad(com_r, pad=(0, self.max_length - com_r.shape[0]))\n",
    "        return (\n",
    "            torch.tensor(q_token['input_ids']), torch.tensor(q_token['token_type_ids']), torch.tensor(q_token['attention_mask']),\n",
    "            torch.tensor(r_token['input_ids']), torch.tensor(r_token['token_type_ids']), torch.tensor(r_token['attention_mask']),\n",
    "            torch.tensor(s),com_q, com_r\n",
    "        )\n",
    "dataset = NLPDataset(dataFrame=dataFrame, tokenizer=tokenizer)\n",
    "trainLoader = DataLoader(dataset, batch_size=4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertExtModel().cuda()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model_opt = torch.optim.AdamW(model.parameters(), 5e-5)\n",
    "lr_sc = torch.optim.lr_scheduler.LinearLR(model_opt, start_factor=0.5, total_iters = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9587/9587 [35:39<00:00,  4.48it/s, AVG_LOSS=0.107, CURRENT_LR=2.5e-5]\n",
      "100%|██████████| 9587/9587 [35:38<00:00,  4.48it/s, AVG_LOSS=0.0465, CURRENT_LR=2.63e-5]\n",
      "100%|██████████| 9587/9587 [35:38<00:00,  4.48it/s, AVG_LOSS=0.045, CURRENT_LR=2.76e-5] \n",
      "100%|██████████| 9587/9587 [35:40<00:00,  4.48it/s, AVG_LOSS=0.043, CURRENT_LR=2.89e-5] \n",
      "100%|██████████| 9587/9587 [35:40<00:00,  4.48it/s, AVG_LOSS=0.043, CURRENT_LR=3.03e-5] \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    total_loss = 0.\n",
    "    currentLR = lr_sc.get_last_lr()[0]\n",
    "    train_process = tqdm(trainLoader)\n",
    "    for batch, data in enumerate(train_process, start = 1):\n",
    "        model_opt.zero_grad()\n",
    "        q_dict = {\"input_ids\" : data[0].cuda(), \"token_type_ids\": data[1].cuda(), \"attention_mask\": data[2].cuda()}\n",
    "        r_dict = {\"input_ids\" : data[3].cuda(), \"token_type_ids\": data[4].cuda(), \"attention_mask\": data[5].cuda()}\n",
    "        s_label, q_label, r_label = data[6], data[7], data[8]\n",
    "        q_pred, r_pred, s_pred = model(q_dict, r_dict)\n",
    "        s_loss = loss_fn(s_pred, s_label.cuda())\n",
    "        q_loss = loss_fn(q_pred.contiguous().reshape(q_pred.shape[0] * q_pred.shape[1], -1), q_label.cuda().contiguous().reshape(-1))\n",
    "        r_loss = loss_fn(r_pred.contiguous().reshape(r_pred.shape[0] * r_pred.shape[1], -1), r_label.cuda().contiguous().reshape(-1))\n",
    "        t_loss = (q_loss + r_loss + s_loss) / 3.\n",
    "        t_loss.backward()\n",
    "        model_opt.step()\n",
    "        total_loss += t_loss.item()\n",
    "        train_process.set_postfix({\"AVG_LOSS\" : total_loss/ batch, \"CURRENT_LR\" : currentLR})\n",
    "    lr_sc.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './bertExtModelFullDataWordGrained.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('bertEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3df01a03ced7e680a43031480445f4f42dc074722258fcaa2e3c400917c2a15b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
